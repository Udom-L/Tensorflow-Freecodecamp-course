{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1IlrlS3bB8t1Gd5Pogol4MIwUxlAjhWOQ","timestamp":1662802720960}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"-ADWvu7NKN2r"},"source":["# __Part 6 - Reinforcement Learning__\n","\n","\n","### __Reinforcement Learning__\n","\n","e.g. training agents (an AI)  interact with environments like games. \n","\n","Rather than feeding our machine learning model millions of examples we __let our model come up with its own examples by exploring an environment.__\n","\n","The concept is simple. __Humans learn by exploring and learning from mistakes and past experiences so let's have our computer do the same.__\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HGCR3JWQLaQb"},"source":["### __Terminology__\n","Before we dive into explaining reinforcement learning we need to define a few key pieces of terminology.\n","\n","- **Environment** In reinforcement learning tasks we have a notion of the environment. __This is what our *agent* will explore.__\n","\n","  - __The environment is essentially defined as the number of states & the number of actions in the way that the agent can interact with given states.__\n","\n","  - An example of an environment in the case of training an AI to play  a game of mario would be __the level we are training the agent on.__\n","\n","- __**Agent** is an entity that is exploring the environment.__ \n","  - Our agent will interact and take different actions within the environment. \n","  - In our mario example the mario character within the game would be our agent. \n","\n","- **State** : our agent will always be in what we call a *state*. __The state simply tells us about the status of the agent (where you are).__\n","  - The most __common example of a state is the location of the agent within the environment.__\n","  - __Moving locations__ would __change the agents state__.\n","\n","- **Action** : __any interaction between the agent and environment would be considered an action.__ \n","  - For example, __moving to the left or jumping would be an action.__ \n","  - An action may or may not change the current *state* of the agent.\n","  - In fact, the act of doing nothing is an action as well! e.g. the action of not pressing a key if we are using our mario example.\n","\n","- **Reward** : __every action that our agent takes will result in a reward of some magnitude (positive or negative).__\n","\n","  - The __goal of our agent__ will be __to maximize its reward in an environment.__ \n","  - Sometimes the reward will be clear, for example if an agent performs an __action which increases their score in the environment__ we could say they've received a __positive reward__. \n","  - If the agent were to perform an __action which results in them losing score or possibly dying in the environment__ then they would receive a __negative reward.__\n","\n","\\\n","\n","__The most important part of reinforcement learning is determing how to reward the agent.__\n","\n","After all, the goal of the agent is to maximize its rewards. This means we should reward the agent appropriately such that it reaches the desired goal.\n","\n","\n","\n","![Fig](https://www.guru99.com/images/1/082319_0514_Reinforceme2.png)"]},{"cell_type":"markdown","metadata":{"id":"AoOJy9s4ZJJt"},"source":["## __Q-Learning__\n","Now that we have a vague idea of how reinforcement learning works it's time to talk about a specific technique in reinforcement learning called *Q-Learning*.\n","\n","__Q-Learning__ is a simple yet quite powerful technique in machine learning that __involves learning a matrix of action-reward values.__ \n","\n","- This matrix is often refered to as a Q-Table or Q-Matrix. The matrix is in shape (number of possible states, number of possible actions).\n","\n","- Each value at matrix[n, m] represents the agents expected reward given they are in state n and take action m. \n","\n","- __The Q-learning algorithm defines the way we update the values in the matrix and decide what action to take at each state.__\n","\n","- The idea is that __after a successful training/learning of this Q-Table/matrix we can determine the action an agent should take in any state__ by looking at that states row in the matrix and __taking the maximium value column as the action__.\n","\n","**Consider this example.**\n","\n","Let's say A1-A4 are the possible actions and we have 3 states represented by each row (state 1 - state 3).\n","\n","   -  | A1  | A2  | A3  | A4  |\n"," :--: |:--: |:--: |:--: |:--: |\n","  S1  |  0  |  0  | 10  |  5  |\n","  S2  |  5  | 10  |  0  |  0  |\n","  S3  | 10  |  5  |  0  |  0  |\n","\n","\n","__Q-values = The numbers inside Q-matrix which represent what the predicted reward will be with specific action (column : A1-A4) while it is in particular state (row : S1-S3).__\n","\n","If that was our Q-Table/matrix then the following would be the prefered actions in each state.\n","\n","> State 1: A3\n","\n","> State 2: A2\n","\n","> State 3: A1\n","\n","We can see that this is because the values in each of those columns are the highest for those states!\n","\n","\n","![alt text](https://i.vas3k.ru/7wa.jpg)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"u5uLpN1yemTx"},"source":["### __Learning the Q-Table__\n","\n","Now how do we create this table and find those values. \n","\n","Well this is where we will discuss how the Q-Learning algorithm updates the values in our Q-Table. \n","\n","I'll start by noting that our Q-Table starts of with all 0 values. This is because the agent has yet to learn anything about the environment. \n","\n","Our agent learns by exploring the environment and observing the outcome/reward from each action it takes in each state. But how does it know what action to take in each state? There are two ways that our agent can decide on which action to take.\n","1. Randomly picking a valid action\n","2. Using the current Q-Table to find the best action.\n","\n","Near the beginning of our agents learning it will mostly take random actions in order to explore the environment and enter many different states. \n","\n","As it starts to explore more of the environment it will start to gradually rely more on it's learned values (Q-Table) to take actions. \n","\n","This means that as our agent explores more of the environment it will develop a better understanding and start to take \"correct\" or better actions more often. It's important that the agent has a good balance of taking random actions and using learned values to ensure it does get trapped in a local maximum. \n","\n","After each new action our agent wil record the new state (if any) that it has entered and the reward that it recieved from taking that action. These values will be used to update the Q-Table. The agent will stop taking new actions only once a certain time limit is reached or it has acheived the goal or reached the end of the environment. \n","\n","\\\n","\n","### __Updating Q-Values__\n","The formula for updating the Q-Table after each action is as follows:\n","> $ Q[state, action] = Q[state, action] + \\alpha * (reward + \\gamma * max(Q[newState, :]) - Q[state, action]) $\n","\n","- $\\alpha$ stands for the **Learning Rate**\n","  - __It is essentially how much we update each cell value__\n","\n","- $\\gamma$ stands for the **Discount Factor**\n","  - __It is defining the balance between finding really good reward in our current state and finding the rewards in the future state.__ \n","  - __The more value, the more looking reward at the future__\n","\n","\\\n","\n","#### __Learning Rate__ $\\alpha$\n","\n","__The learning rate $\\alpha$__ __is a numeric constant that defines how much change is permitted on each QTable update.__ \n","\n","  - __High learning rate__ means that __each update will introduce a large change to the current state-action value.__\n","\n","  -__Small learning rate__ means that __each update has a more subtle change.__ \n","  \n","  - __Modifying the learning rate__ will __change__ how the __agent explores the environment and how quickly it determines the final values in the QTable__.\n","\n","\\\n","\n","#### __Discount Factor $\\gamma$__\n","\n","__Discount factor ($\\gamma$)__ is used to balance how much focus is put on the current and future reward. \n","\n","  - __High discount factor__ means that __future rewards__ will be considered __more heavily.__\n","\n","\\\n","\n","To perform updates on this table we will __let the agent explore the environment__ for a certain period of time __and use each of its actions to make an update__. Slowly we should __start to notice the agent learning and choosing better actions.__ \n","\n","\\\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rwIl0sJgmu4D"},"source":["## __Q-Learning Example__\n","\n","For this example we will use the Q-Learning algorithm to train an agent to navigate a popular environment from the [Open AI Gym](https://gym.openai.com/). The Open AI Gym was developed so programmers could practice machine learning using unique environments. Intersting fact, Elon Musk is one of the founders of OpenAI!\n","\n","Let's start by looking at what Open AI Gym is. "]},{"cell_type":"code","metadata":{"id":"rSETF0zqokYr"},"source":["import gym   # all you have to do to import and use open ai gym!"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8cH3AmCzotO1"},"source":["Once you import gym you can load an environment using the line ```gym.make(\"environment\")```."]},{"cell_type":"code","metadata":{"id":"UKN1ScBco3dp"},"source":["env = gym.make('FrozenLake-v1')  # we are going to use the FrozenLake environment"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3SvSlmVwo8cY"},"source":["There are a few other commands that can be used to interact and get information about the environment."]},{"cell_type":"code","metadata":{"id":"FF3icIeapFct","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665779275975,"user_tz":-120,"elapsed":10,"user":{"displayName":"Udomlerd Srisuchinwong","userId":"02064572073666561987"}},"outputId":"17474c9a-1641-4191-cb92-36a9ea040377"},"source":["print(env.observation_space.n)   # get number of states\n","print(env.action_space.n)   # get number of actions"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["16\n","4\n"]}]},{"cell_type":"code","metadata":{"id":"lc9cwp03pQVn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665779276950,"user_tz":-120,"elapsed":7,"user":{"displayName":"Udomlerd Srisuchinwong","userId":"02064572073666561987"}},"outputId":"ee51d7ab-a178-48e1-e676-37076f71c5ad"},"source":["env.reset()  # reset environment to default state"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","metadata":{"id":"sngyjPDapUt7"},"source":["action = env.action_space.sample()  # get a random action "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HeEfi8xypXya"},"source":["new_state, reward, done, info = env.step(action)  # take action, notice it returns information about the action"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_1W3D81ipdaS"},"source":["# env.render()   # render the GUI for the environment "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vmW6HAbQp01f"},"source":["###__Frozen Lake environment__\n","Now that we have a basic understanding of how the gym environment works it's time to discuss the specific problem we will be solving.\n","\n","The environment we loaded above ```FrozenLake-v0``` is one of the simplest environments in Open AI Gym. The goal of the agent is to navigate a frozen lake and find the Goal without falling through the ice (render the environment above to see an example).\n","\n","There are:\n","- 16 states (one for each square) \n","- 4 possible actions (LEFT, RIGHT, DOWN, UP)\n","- 4 different types of blocks (F: frozen, H: hole, S: start, G: goal)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"YlWoK75ZrK2b"},"source":["###__Building the Q-Table__\n","The first thing we need to do is build an empty Q-Table that we can use to store and update our values."]},{"cell_type":"code","metadata":{"id":"r767K4s0rR2p"},"source":["import gym\n","import numpy as np\n","import time\n","\n","env = gym.make('FrozenLake-v1')\n","STATES = env.observation_space.n\n","ACTIONS = env.action_space.n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UAzMWGatrVIk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665779279638,"user_tz":-120,"elapsed":9,"user":{"displayName":"Udomlerd Srisuchinwong","userId":"02064572073666561987"}},"outputId":"1d942eef-4ae9-4cb3-db4f-92d08124142e"},"source":["Q = np.zeros((STATES, ACTIONS))  # create a matrix with all 0 values \n","Q"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.],\n","       [0., 0., 0., 0.]])"]},"metadata":{},"execution_count":26}]},{"cell_type":"markdown","metadata":{"id":"vc_h8tLSrpmc"},"source":["###__Constants__\n","As we discussed we need to define some constants that will be used to update our Q-Table and tell our agent when to stop training."]},{"cell_type":"code","metadata":{"id":"-FQapdnnr6P1"},"source":["EPISODES = 2000 # how many times to run the environment from the beginning\n","MAX_STEPS = 100  # max number of steps allowed for each run of environment\n","\n","LEARNING_RATE = 0.81  # learning rate\n","GAMMA = 0.96          # discount factor"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NxrAj91rsMfm"},"source":["###__Picking an Action__\n","Remember that we can pick an action using one of two methods:\n","1. Randomly picking a valid action\n","2. Using the current Q-Table to find the best action.\n","\n","Here we will define a new value $\\epsilon$ that will tell us the probabillity of selecting a random action. This value will start off very high and slowly decrease as the agent learns more about the environment."]},{"cell_type":"code","metadata":{"id":"YUAQVyX0sWDb"},"source":["epsilon = 0.9  # start with a 90% chance of picking a random action\n","\n","# code to pick action\n","if np.random.uniform(0, 1) < epsilon:  # we will check if a randomly selected value is less than epsilon.\n","    action = env.action_space.sample()  # take random action\n","else:\n","    action = np.argmax(Q[state, :])  # use Q table to pick best action based on current values"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5n-i0B7Atige"},"source":["###__Updating Q Values__'\n","The code below implements the formula discussed above."]},{"cell_type":"code","metadata":{"id":"9r7R1W6Qtnh8"},"source":["Q[state, action] = Q[state, action] + LEARNING_RATE * (reward + GAMMA * np.max(Q[new_state, :]) - Q[state, action])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"__afaD62uh8G"},"source":["###__Putting it Together__\n","Now that we know how to do some basic things we can combine these together to create our Q-Learning algorithm,"]},{"cell_type":"code","metadata":{"id":"AGiYCiNuutHz"},"source":["import gym\n","import numpy as np\n","import time\n","\n","env = gym.make('FrozenLake-v1')\n","STATES = env.observation_space.n\n","ACTIONS = env.action_space.n\n","\n","Q = np.zeros((STATES, ACTIONS))\n","\n","EPISODES = 1500 # how many times to run the environment from the beginning\n","MAX_STEPS = 100  # max number of steps allowed for each run of environment\n","\n","LEARNING_RATE = 0.81  # learning rate\n","GAMMA = 0.96\n","\n","RENDER = False # if you want to see training set to true\n","\n","epsilon = 0.9\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jFRtn5dUu5ZI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1665779284952,"user_tz":-120,"elapsed":2938,"user":{"displayName":"Udomlerd Srisuchinwong","userId":"02064572073666561987"}},"outputId":"96913e8a-2d0e-45f2-a9ff-44b69382117d"},"source":["rewards = []\n","for episode in range(EPISODES):\n","\n","  state = env.reset()\n","  for _ in range(MAX_STEPS):\n","    \n","    if RENDER:\n","      env.render()\n","    \n","    if np.random.uniform(0, 1) < epsilon: # Take action\n","      action = env.action_space.sample()  \n","    else:\n","      action = np.argmax(Q[state, :])\n","\n","    next_state, reward, done, _ = env.step(action) # After action, store in next_state and reward\n","\n","    Q[state, action] = Q[state, action] + LEARNING_RATE * (reward + GAMMA * np.max(Q[next_state, :]) - Q[state, action])\n","\n","    state = next_state\n","\n","    if done: \n","      rewards.append(reward)\n","      epsilon -= 0.001\n","      break  # reached goal\n","\n","print(Q)\n","print(f\"Average reward: {sum(rewards)/len(rewards)}:\")\n","# and now we can see our Q values!"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[2.89199936e-01 1.00042470e-02 7.49341365e-03 9.69907356e-03]\n"," [1.31018159e-03 3.53624695e-03 3.54446770e-03 1.96329889e-01]\n"," [2.33208278e-03 1.41894768e-01 2.07001713e-03 2.89800191e-03]\n"," [2.59299981e-03 2.29818435e-03 1.47272856e-03 9.62339446e-02]\n"," [3.23789212e-01 8.62636978e-03 2.86358403e-03 7.77106627e-03]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [9.08474562e-06 1.83057965e-04 1.38604496e-05 5.21872960e-06]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [6.66293665e-03 6.89279322e-03 6.74331493e-03 5.02662554e-01]\n"," [6.18177767e-03 7.84156227e-01 1.96142754e-03 1.36898264e-03]\n"," [1.55795639e-01 2.53732628e-04 1.06844196e-03 3.56201285e-04]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n"," [1.22547379e-01 3.07935424e-02 7.70138277e-01 1.28239123e-01]\n"," [1.87373258e-01 8.04031829e-01 7.54530118e-02 1.38967880e-01]\n"," [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n","Average reward: 0.2806666666666667:\n"]}]},{"cell_type":"code","metadata":{"id":"Zo-tNznd65US","colab":{"base_uri":"https://localhost:8080/","height":279},"executionInfo":{"status":"ok","timestamp":1665779284953,"user_tz":-120,"elapsed":12,"user":{"displayName":"Udomlerd Srisuchinwong","userId":"02064572073666561987"}},"outputId":"6d81858b-4f95-4911-f158-a96fb06410dc"},"source":["# we can plot the training progress and see how the agent improved\n","import matplotlib.pyplot as plt\n","\n","def get_average(values):\n","  return sum(values)/len(values)\n","\n","avg_rewards = []\n","for i in range(0, len(rewards), 100):\n","  avg_rewards.append(get_average(rewards[i:i+100])) \n","\n","plt.plot(avg_rewards)\n","plt.ylabel('average reward')\n","plt.xlabel('episodes (100\\'s)') # how many times to run the environment from the beginning\n","plt.show()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"gy4YH2m9s1ww"},"source":["##Sources\n","1. Violante, Andre. “Simple Reinforcement Learning: Q-Learning.” Medium, Towards Data Science, 1 July 2019, https://towardsdatascience.com/simple-reinforcement-learning-q-learning-fcddc4b6fe56.\n","\n","2. Openai. “Openai/Gym.” GitHub, https://github.com/openai/gym/wiki/FrozenLake-v0."]}]}